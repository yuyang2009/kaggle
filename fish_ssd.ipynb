{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from PIL import Image \n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "from preprocessing import inception_preprocessing\n",
    "from nets import inception\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
    "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "    dataset, common_queue_capacity=batch_size*3,\n",
    "    common_queue_min=8)\n",
    "    \n",
    "    image_raw, label, bboxes = data_provider.get(['image', 'label', 'bbox'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training, bbox=bboxes)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels, bboxes = tf.train.batch(\n",
    "          [image, image_raw, label, bboxes],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size,\n",
    "          dynamic_pad=True)\n",
    "    \n",
    "    return images, images_raw, labels, bboxes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Provides data for the fish dataset.\n",
    "\n",
    "The dataset scripts used to create the dataset can be found at:\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "_FILE_PATTERN = 'fish_%s_*.tfrecord'\n",
    "\n",
    "SPLITS_TO_SIZES = {'train': 11002, 'validation': 2750, 'test': 1000}\n",
    "\n",
    "_NUM_CLASSES = 8\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'image': 'A color image of varying size.',\n",
    "    'label': 'A single integer between 0 and 7',\n",
    "}\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading fish.\n",
    "\n",
    "    Args:\n",
    "    split_name: A train/validation split name.\n",
    "    dataset_dir: The base directory of the dataset sources.\n",
    "    file_pattern: The file pattern to use when matching the dataset sources.\n",
    "      It is assumed that the pattern contains a '%s' string so that the split\n",
    "      name can be inserted.\n",
    "    reader: The TensorFlow reader type.\n",
    "\n",
    "    Returns:\n",
    "    A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if `split_name` is not a valid train/validation split.\n",
    "    \"\"\"\n",
    "    if split_name not in SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "        file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        'image/bbox/ymin': tf.VarLenFeature(tf.float32),\n",
    "        'image/bbox/xmin': tf.VarLenFeature(tf.float32),\n",
    "        'image/bbox/ymax': tf.VarLenFeature(tf.float32),\n",
    "        'image/bbox/xmax': tf.VarLenFeature(tf.float32),\n",
    "    }\n",
    "\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "        'bbox': slim.tfexample_decoder.BoundingBox(\n",
    "            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/bbox/'),\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    labels_to_names = None\n",
    "    if dataset_utils.has_labels(dataset_dir):\n",
    "        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=reader,\n",
    "        decoder=decoder,\n",
    "        num_samples=SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        labels_to_names=labels_to_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convnet visualize\n",
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# image_size = inception.inception_v3.default_image_size\n",
    "# image_size-Mixed_7c : 512-14\n",
    "# image_size-Mixed_7b : 512-14\n",
    "# image_size-Mixed_6e : 512-30\n",
    "# image_size-Mixed_5d : 512-61\n",
    "image_size = 720\n",
    "batch_size = 8\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "train_dir = 'tmp/fish/inception_finetuned/'\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('validation', fish_data_dir)\n",
    "    images, images_raw, labels, bboxes = load_batch(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        _, _end_points = inception.inception_v3_base(images)\n",
    "#          _, _convnet = inception.inception_v3_base(images)\n",
    "            \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            init_fn(sess)\n",
    "            np_end_points, np_images_raw, np_bboxes = sess.run([_end_points, images_raw, bboxes])\n",
    "            plt.rcParams['figure.figsize'] = (30.0, 15.0)\n",
    "#             for i in range(batch_size): \n",
    "#                 image = np_images_raw[i, :, :, :]\n",
    "#                 convnet = np_end_points['Mixed_5d'][i]\n",
    "#                 vis_square(convnet.transpose(2, 0, 1), padval=0.5)\n",
    "                \n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(image.astype(np.uint8))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that this may take several minutes!!\n",
    "# Fine-tune pre-trained netoworks.\n",
    "\n",
    "import os\n",
    "\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV3/Logits\", \"InceptionV3/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "    \n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'inception_v3.ckpt'),\n",
    "        variables_to_restore)\n",
    "\n",
    "\n",
    "train_dir = 'tmp/fish/inception_ssd/'\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "checkpoints_dir = 'tmp/fish/my_checkpoints'\n",
    "batch_size = 16\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('train', fish_data_dir)\n",
    "    \n",
    "    images, _, labels, bboxes = load_batch(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        _, _end_points = inception.inception_v3_base(images)\n",
    "        \n",
    "#     # Specify the loss function:\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "#     slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "#     total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # image_size - Mixed_7c : 720 - 21x21x2048\n",
    "    # image_size - Mixed_7b : 720 - 21x21x2048\n",
    "    # image_size - Mixed_6e : 720 - 43x43x768\n",
    "    # image_size - Mixed_5d : 720 - 87x87x228\n",
    "    \n",
    "#     Mixed_5d = _end_points['Mixed_5d']\n",
    "    \n",
    "    num_default_bbox=5\n",
    "    output_pre_class = (dataset.num_classes+4)*num_default_bbox\n",
    "    \n",
    "    Mixed_6e = _end_points['Mixed_6e']\n",
    "    Mixed_6e_ssd = slim.conv2d(Mixed_6e, output_pre_class, [3 ,3] )\n",
    "    \n",
    "    Mixed_7b = _end_points['Mixed_7b']\n",
    "    Mixed_7b_ssd = slim.conv2d(Mixed_7b, 1024, [3, 3])\n",
    "    Mixed_7b_ssd = slim.conv2d(Mixed_7b_ssd, output_pre_class, [3 ,3] )\n",
    "\n",
    "    #21\n",
    "    Mixed_7c = _end_points['Mixed_7c']\n",
    "    Mixed_7c_ssd = slim.conv2d(Mixed_7c, 1024, [1, 1])\n",
    "    Mixed_7c_ssd = slim.conv2d(Mixed_6e, output_pre_class, [3 ,3] )\n",
    "    \n",
    "    #11\n",
    "    Conv_8a_ssd = slim.conv2d(Mixed_7c_ssd, 256, [1, 1])\n",
    "    Conv_8b_ssd = slim.conv2d(Conv_8a_ssd, 512, [3, 3], stride=2)\n",
    "\n",
    "    #6\n",
    "    Conv_9a_ssd = slim.conv2d(Conv_8b_ssd, 128, [1, 1])\n",
    "    Conv_9b_ssd = slim.conv2d(Conv_9a_ssd, 256, [3, 3], stride=2)\n",
    "\n",
    "    #3\n",
    "    Conv_10a_ssd = slim.conv2d(Conv_9b_ssd, 128, [1, 1])\n",
    "    Conv_10b_ssd = slim.conv2d(Conv_10a_ssd, 256, [3, 3], stride=2)\n",
    "    \n",
    "    #1\n",
    "    pool_11_ssd = slim.pool(Conv_10b_ssd, [3, 3], pooling_type=\"AVG\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    # GPU error on incepton-v3 because of motherboard: \"failed to query event: CUDA_ERROR_LAUNCH_FAILED,Unexpected Event status: 1\"\n",
    "\n",
    "#     config = tf.ConfigProto(device_count = {'gpu': 0})\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "# windows 查看tensorflow/contrib/slim/python/slim/learning.py 中的 start_standard_services = False\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        init_fn=get_init_fn(),\n",
    "        number_of_steps=5,\n",
    "        session_config=config,)\n",
    "        \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "    dataset, common_queue_capacity=batch_size*3,\n",
    "    common_queue_min=8)\n",
    "    \n",
    "    image_raw, label, bboxes = data_provider.get(['image', 'label', 'bbox'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image, distorted_bbox = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=True)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels, bboxes, distorted_bbox = tf.train.batch(\n",
    "          [image, image_raw, label, bboxes, distorted_bbox],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size,\n",
    "          dynamic_pad=True)\n",
    "    \n",
    "    return images, images_raw, labels, bboxes, distorted_bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = 720\n",
    "batch_size = 8\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('validation', fish_data_dir)\n",
    "    images, images_raw, labels, bboxes, distorted_bbox = load_batch(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            np_images_raw, np_bboxes, np_distorted_bbox = sess.run([images_raw, bboxes, distorted_bbox])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "plt.rcParams['figure.figsize'] = (30.0, 15.0)\n",
    "ind = 3\n",
    "plt.figure()\n",
    "plt.imshow(np_images_raw[ind].astype(np.uint8))\n",
    "width = 720\n",
    "height = 720\n",
    "currentAxis = plt.gca()\n",
    "j = 0\n",
    "for rect in np_bboxes[ind]:\n",
    "    [ymin, xmin, ymax, xmax] = rect\n",
    "    coords = (xmin*width, ymin*height), (xmax-xmin)*width, (ymax-ymin)*height\n",
    "    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=colors[j], linewidth=2))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bboxes_jaccard(bbox_ref, bboxes, name=None):\n",
    "    \"\"\"Compute jaccard score between a reference box and a collection\n",
    "    of bounding boxes.\n",
    "\n",
    "    Args:\n",
    "      bbox_ref: (N, 4) or (4,) Tensor with reference bounding box(es).\n",
    "      bboxes: (N, 4) Tensor, collection of bounding boxes.\n",
    "    Return:\n",
    "      (N,) Tensor with Jaccard scores.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, 'bboxes_jaccard'):\n",
    "        # Should be more efficient to first transpose.\n",
    "        bboxes = tf.transpose(bboxes)\n",
    "        bbox_ref = tf.transpose(bbox_ref)\n",
    "        # Intersection bbox and volume.\n",
    "        int_ymin = tf.maximum(bboxes[0], bbox_ref[0])\n",
    "        int_xmin = tf.maximum(bboxes[1], bbox_ref[1])\n",
    "        int_ymax = tf.minimum(bboxes[2], bbox_ref[2])\n",
    "        int_xmax = tf.minimum(bboxes[3], bbox_ref[3])\n",
    "        h = tf.maximum(int_ymax - int_ymin, 0.)\n",
    "        w = tf.maximum(int_xmax - int_xmin, 0.)\n",
    "        # Volumes.\n",
    "        inter_vol = h * w\n",
    "        union_vol = -inter_vol \\\n",
    "            + (bboxes[2] - bboxes[0]) * (bboxes[3] - bboxes[1]) \\\n",
    "            + (bbox_ref[2] - bbox_ref[0]) * (bbox_ref[3] - bbox_ref[1])\n",
    "        jaccard = tfe_math.safe_divide(inter_vol, union_vol, 'jaccard')\n",
    "        return jaccard\n",
    "    \n",
    "def bboxes_resize(bbox_ref, bboxes, name=None):\n",
    "    \"\"\"Resize bounding boxes based on a reference bounding box,\n",
    "    assuming that the latter is [0, 0, 1, 1] after transform. Useful for\n",
    "    updating a collection of boxes after cropping an image.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, 'bboxes_resize'):\n",
    "        # Translate.\n",
    "        v = tf.stack([bbox_ref[0], bbox_ref[1], bbox_ref[0], bbox_ref[1]])\n",
    "        bboxes = bboxes - v\n",
    "        # Scale.\n",
    "        s = tf.stack([bbox_ref[2] - bbox_ref[0],\n",
    "                      bbox_ref[3] - bbox_ref[1],\n",
    "                      bbox_ref[2] - bbox_ref[0],\n",
    "                      bbox_ref[3] - bbox_ref[1]])\n",
    "        bboxes = bboxes / s\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 1, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_distorted_bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
