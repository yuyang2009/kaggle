{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from PIL import Image \n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width_images = 100\n",
    "height_images = 40\n",
    "num_charmap = 36\n",
    "num_channel = 1\n",
    "num_char = 4\n",
    "char_map = string.ascii_uppercase + string.digits\n",
    "\n",
    "def img_preprocess(img):\n",
    "    img_resize = np.array(img.resize((width_images, height_images)))\n",
    "#     img_gray = np.mean(img_resize, -1)\n",
    "    img_gray = np.dot(img_resize, [0.2989, 0.5870, 0.1140])\n",
    "    img_scale = np.multiply(img_gray, 1/255.0)\n",
    "    img_func = np.vectorize(lambda x: x if (x<0.7) else 1)\n",
    "    return img_func(img_scale)\n",
    "\n",
    "\n",
    "def dataset_generator(num_images):\n",
    "    image_generator = ImageCaptcha(width=100, height=40, fonts=['./font/LucidaGrande.ttf'], font_sizes=[38])\n",
    "    X = np.zeros((num_images, height_images, width_images))\n",
    "    y = np.zeros((num_images, num_charmap*num_char))\n",
    "    for i in range(num_images):\n",
    "        text = ''.join(random.sample(char_map, num_char))\n",
    "        img = image_generator.generate_image(text)\n",
    "        X[i, :, :] = img_preprocess(img)\n",
    "        y_index = [ char_map.find(text[_i])+_i*num_charmap for _i in range(num_char)]\n",
    "        y[i, y_index] = 1\n",
    "\n",
    "    return X, y\n",
    "    \n",
    "def get_next_batch(X, y, batch_size=100):\n",
    "    index_in_epoch = 0\n",
    "    num_images = X.shape[0]\n",
    "#     assert num_images <= X.shape[0]\n",
    "    while True:\n",
    "        start = index_in_epoch\n",
    "        index_in_epoch += batch_size\n",
    "        if index_in_epoch > num_images:\n",
    "            perm = np.arange(num_images)\n",
    "            np.random.shuffle(perm)\n",
    "            X = X[perm]\n",
    "            y = y[perm]\n",
    "            start = 0\n",
    "            index_in_epoch = batch_size\n",
    "            assert batch_size <= num_images\n",
    "        end = index_in_epoch\n",
    "        yield X[start:end], y[start:end]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#参数初始化研究\n",
    "\n",
    "def weight_variable(shape):\n",
    "#     initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "#     return tf.Variable(initial)\n",
    "    return tf.Variable(0.01*tf.random_normal(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "#     initial = tf.constant(0.1, shape=shape)\n",
    "#     return tf.Variable(initial)\n",
    "    return tf.Variable(0.1*tf.random_normal(shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, height_images, width_images])\n",
    "y_ = tf.placeholder(tf.float32, [None, num_char,  num_charmap])\n",
    "\n",
    "#todo: find out why reshape doesn.t work\n",
    "x_image = tf.reshape(x, [-1, height_images, width_images, 1])\n",
    "# y_ = tf.reshape(y_, [-1, num_char, num_charmap])\n",
    "\n",
    "#Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "#First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# h_pool1_drop = tf.nn.dropout(h_pool1, keep_prob)\n",
    "\n",
    "#Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# h_pool2_drop = tf.nn.dropout(h_pool2, keep_prob)\n",
    "\n",
    "#Third Convolutional Layer\n",
    "W_conv3 = weight_variable([5, 5, 64, 64])\n",
    "b_conv3 = bias_variable([64])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "#Fourth Convolutional Layer\n",
    "# W_conv4 = weight_variable([3, 3, 64, 64])\n",
    "# b_conv4 = bias_variable([64])\n",
    "# h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "# h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "#First densely Connected Layer\n",
    "# W_fc1 = weight_variable([10 * 25 * 64, 1024])\n",
    "W_fc1 = weight_variable([5 * 13 * 64, 1024])\n",
    "# W_fc1 = weight_variable([3 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, W_fc1.get_shape().as_list()[0]])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Readout Layer\n",
    "W_fc2 = weight_variable([1024, 144])\n",
    "b_fc2 = bias_variable([144])\n",
    "\n",
    "y_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "#cross_entropy_1 无法导向正确结果，废弃\n",
    "# cross_entropy_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, tf.reshape(y_, [-1, num_char*num_charmap])))\n",
    "#分别计算每个char的softmax损失函数，取其平均值作为整体的损失函数\n",
    "\n",
    "y_conv = tf.reshape(y_fc2, [-1, num_char, num_charmap])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.concat(0, [tf.nn.softmax_cross_entropy_with_logits(y_conv[:, _i, :], y_[:, _i, :]) for _i in range(num_char) ]))\n",
    "\n",
    "# 训练过程中cross_entropy_1反而会增大\n",
    "# train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy_1)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "max_idx_p = tf.argmax(y_conv, 2)\n",
    "max_idx_l = tf.argmax(y_, 2)\n",
    "correct_prediction = tf.cast(tf.equal(max_idx_p, max_idx_l), tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.reduce_min(correct_prediction, axis=1))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "# with tf.Session(config=config) as sess:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#batch_size 小会导致多次迭代仍无法收敛\n",
    "#h_fc1_drop：one batch 64:100; 128:200; 256:800\n",
    "#h_fc1_drop：all batches 64:++; 128:8000; 256:6900\n",
    "#h_fc1_drop, h_conv2_drop: one batch 64:700; 128:800; 256:1200\n",
    "#h_fc1_drop, h_conv2_drop: all batches 64:++; 128:++; 256:++\n",
    "#每当训练准确率达到90%，更换训练集，直到测试准确率达到80%\n",
    "#1W-0.028; 3W-0.269\n",
    "\n",
    "num_images_train = 20000\n",
    "num_images_test = 1000\n",
    "test_accuracy = 0.0\n",
    "epoch = 0\n",
    "while True:\n",
    "    saver.restore(sess, \"./model/model.ckpt\")\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    print(\"generate train dataset: \\n\", '-'*40)\n",
    "    X_train, y_train = dataset_generator(num_images_train)\n",
    "    print(\"generate test dataset: \\n\", '-'*40)\n",
    "    X_test, y_test = dataset_generator(num_images_test)\n",
    "    y_test = y_test.reshape(-1, num_char, num_charmap)\n",
    "    #变化的batch_size训练更快\n",
    "    batch_size= 64*(epoch+1)\n",
    "    train_batch = get_next_batch(X_train, y_train, batch_size)\n",
    "    step = 0\n",
    "    # X_batch, y_batch = next(train_batch)\n",
    "    # y_batch = y_batch.reshape(-1, num_char, num_charmap)\n",
    "    test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test, keep_prob: 1.0})\n",
    "    print(time.ctime() ,\"epoch %d,test accuracy %g\"%(epoch, test_accuracy))\n",
    "    print(\"* \"*40)\n",
    "    while True:\n",
    "        X_batch, y_batch = next(train_batch)\n",
    "        y_batch = y_batch.reshape(-1, num_char, num_charmap)\n",
    "        if (step+1)%500 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(time.ctime() ,\": step %d, training accuracy %g\"%(step, train_accuracy))\n",
    "            #过拟合，训练准确率不能超过 95%\n",
    "            #train_ac - test_ac > 0.08\n",
    "            if train_accuracy >= 0.95:\n",
    "                test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test, keep_prob: 1.0})\n",
    "                print(time.ctime() ,\"epoch %d,test accuracy %g\"%(epoch, test_accuracy))\n",
    "                print(\"* \"*40)\n",
    "                saver.save(sess, \"./model/model.ckpt\")\n",
    "                break\n",
    "        step += 1\n",
    "        sess.run(train_step, feed_dict={x: X_batch, y_: y_batch, keep_prob: 0.5})\n",
    "    if test_accuracy >= 0.88 or batch_size >= 512:\n",
    "        print(time.ctime() ,\"training done!test accuracy is %g\"%test_accuracy)\n",
    "        break\n",
    "    epoch += 1\n",
    "    \n",
    "#  无法收敛的元凶:batch_size\n",
    "#     train_step.run(feed_dict={x: X_batch, y_: y_batch, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training 2\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "step = 0\n",
    "# X_train, y_train = dataset_generator(20000)\n",
    "train_batch = get_next_batch(X_train, y_train, batch_size)\n",
    "\n",
    "# X_test, y_test = dataset_generator(500)\n",
    "y_test = y_test.reshape((-1, num_char, num_charmap))\n",
    "while True:\n",
    "    X_batch, y_batch = next(train_batch)\n",
    "    y_batch = y_batch.reshape((-1, num_char, num_charmap))\n",
    "    \n",
    "    if (step)%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        loss_val = cross_entropy.eval(feed_dict={x: X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "        test_accuracy = accuracy.eval(feed_dict={x:X_test, y_: y_test, keep_prob: 1.0})\n",
    "        print(time.ctime() ,\": step %d, train accuracy: %g\"%(step, train_accuracy), \";test accuracy: \", test_accuracy)\n",
    "        print(\"loss: \", loss_val)\n",
    "        if step >= 5000:\n",
    "            break\n",
    "    step += 1\n",
    "    sess.run(train_step, feed_dict={x: X_batch, y_: y_batch, keep_prob: 0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession(config=config)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver.restore(sess, \"./model/model.ckpt\")\n",
    "# print(\"generate test dataset: \\n\", '-'*40)\n",
    "# from captcha.image import ImageCaptcha\n",
    "# image_generator = ImageCaptcha(width=100, height=40, fonts=['./font/LucidaGrande.ttf'], font_sizes=[38])\n",
    "X_test, y_test = dataset_generator(500)\n",
    "y_test = y_test.reshape(-1, num_char, num_charmap)\n",
    "X, y = X_test, y_test\n",
    "pred = max_idx_p.eval(feed_dict={x:X, keep_prob: 1.0})\n",
    "print(\"test accuracy: \",accuracy.eval(feed_dict={x: X, y_: y, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with tf.Session(config=config) as sess:\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.InteractiveSession(config=config)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver.restore(sess, \"./model/model.ckpt\")\n",
    "plot_ind = 50\n",
    "idx_lst = range(plot_ind, plot_ind+20)\n",
    "for idx in idx_lst:\n",
    "#     print(''.join([char_map[_i] for _i in pred[idx]]))\n",
    "    text = ''.join([char_map[_i] for _i in pred[idx]])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.text(0.1, 1.1, text, ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "    plt.imshow(X[idx].reshape((height_images, width_images)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_set_ge():\n",
    "    for _i in range(1, 6, 1):\n",
    "        folder = 'part' + str(_i)\n",
    "        imgpath = './captcha_train/captcha1/' + folder + '/*.jpg'\n",
    "        train_data_lst =  glob.glob(imgpath)\n",
    "        batch_size = 20000\n",
    "        X_train = np.zeros((batch_size, height_images, width_images))\n",
    "        y_train = np.zeros((batch_size, num_charmap*num_char))\n",
    "        _indx = 0\n",
    "        for _img in train_data_lst:\n",
    "            img = Image.open(_img)\n",
    "            img_scale = img_preprocess(img)\n",
    "            X_train[_indx, :, :] = img_scale\n",
    "            text = _img.split('\\\\')[-1].split('#')[-1].split('.')[0]\n",
    "            y_indx = [ char_map.find(text[_i])+_i*num_charmap for _i in range(num_char)]\n",
    "            y_train[_indx, y_indx] = 1\n",
    "            _indx += 1\n",
    "        yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_g = train_set_ge()\n",
    "X, y = next(t_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training 3\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "# with tf.Session(config=config) as sess:\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#batch_size 小会导致多次迭代仍无法收敛\n",
    "#h_fc1_drop：one batch 64:100; 128:200; 256:800\n",
    "#h_fc1_drop：all batches 64:++; 128:8000; 256:6900\n",
    "#h_fc1_drop, h_conv2_drop: one batch 64:700; 128:800; 256:1200\n",
    "#h_fc1_drop, h_conv2_drop: all batches 64:++; 128:++; 256:++\n",
    "#每当训练准确率达到90%，更换训练集，直到测试准确率达到80%\n",
    "#1W-0.028; 3W-0.269\n",
    "\n",
    "num_images_train = 20000\n",
    "epoch = 0\n",
    "t_g = train_set_ge()\n",
    "saver.restore(sess, \"./model/model.ckpt\")\n",
    "while True:\n",
    "    X_train, y_train = next(t_g)\n",
    "    #变化的batch_size训练更快\n",
    "    batch_size= 64*(epoch+1)\n",
    "    train_batch = get_next_batch(X_train, y_train, batch_size)\n",
    "    step = 0\n",
    "    while True:\n",
    "        X_batch, y_batch = next(train_batch)\n",
    "        y_batch = y_batch.reshape(-1, num_char, num_charmap)\n",
    "        if (step+1)%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:X_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(time.ctime() ,\": step %d, training accuracy %g\"%(step, train_accuracy))\n",
    "            if train_accuracy >= 0.90:\n",
    "                break\n",
    "        step += 1\n",
    "        sess.run(train_step, feed_dict={x: X_batch, y_: y_batch, keep_prob: 0.5})\n",
    "    epoch += 1\n",
    "saver.save(sess, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.save(sess, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itable_lst =  glob.glob('./captcha/*')\n",
    "X_itable = np.empty((len(itable_lst), height_images, width_images))\n",
    "_ind = 0\n",
    "img_func = np.vectorize(lambda x: x if (x<0.7) else 1)\n",
    "for img_name in itable_lst:\n",
    "    img = Image.open(img_name)\n",
    "    img_scale = img_preprocess(img)\n",
    "    X_itable[_ind, :, :] = img_scale\n",
    "    _ind += 1\n",
    "# saver = tf.train.Saver()\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.InteractiveSession(config=config)\n",
    "# saver.restore(sess, \"./model/model.ckpt\")\n",
    "X = X_itable\n",
    "pred = max_idx_p.eval(feed_dict={x:X, keep_prob: 1.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ind = 80\n",
    "idx_lst = range(plot_ind, plot_ind+20)\n",
    "for idx in idx_lst:\n",
    "#     print(''.join([char_map[_i] for _i in pred[idx]]))\n",
    "    text = ''.join([char_map[_i] for _i in pred[idx]])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.text(0.1, 1.1, text, ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "    plt.imshow(X[idx].reshape((height_images, width_images)), cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itable_lst =  glob.glob('./captcha/*')\n",
    "X_itable = np.empty((len(itable_lst), height_images, width_images))\n",
    "_ind = 0\n",
    "for img_name in itable_lst:\n",
    "    if _ind < 10:\n",
    "        img = Image.open(img_name)\n",
    "        plt.figure()\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "    _ind += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(len(X_itable)):\n",
    "    text = ''.join([char_map[_i] for _i in pred[idx]])\n",
    "    plt.imsave('./captcha_train/pred/'+str(idx)+'_'+text+'.jpg', X_itable[idx], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, y_test = dataset_generator(10)\n",
    "y_test = y_test.reshape(-1, num_char, num_charmap)\n",
    "y_ind = np.argmax(y_test, axis=2)\n",
    "text = [''.join([char_map[_i] for _i in _ind]) for _ind in y_ind]\n",
    "print(text)\n",
    "for i in range(10):\n",
    "    plt.imsave('./img/'+str(i)+'.jpg', X_test[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from captcha.image import ImageCaptcha\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from PIL import Image \n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width_images = 100\n",
    "height_images = 40\n",
    "num_charmap = 36\n",
    "num_channel = 1\n",
    "num_char = 4\n",
    "char_map = string.ascii_uppercase + string.digits\n",
    "\n",
    "def img_preprocess(img):\n",
    "    img_resize = np.array(img.resize((width_images, height_images)))\n",
    "#     img_gray = np.mean(img_resize, -1)\n",
    "    img_gray = np.dot(img_resize, [0.2989, 0.5870, 0.1140])\n",
    "    img_scale = np.multiply(img_gray, 1/255.0)\n",
    "    img_func = np.vectorize(lambda x: x if (x<0.7) else 1)\n",
    "    return img_func(img_scale).reshape((-1, width_images*height_images))\n",
    "#     return img_scale\n",
    "\n",
    "def dataset_generator(num_images):\n",
    "#     image_generator = ImageCaptcha(fonts=['./font/AntykwaBold.ttf'])\n",
    "    image_generator = ImageCaptcha(width=100, height=40, fonts=['./font/LucidaGrande.ttf'], font_sizes=[38])\n",
    "    X = np.zeros((num_images, height_images*width_images))\n",
    "    y = np.zeros((num_images, num_charmap*num_char))\n",
    "    for i in range(num_images):\n",
    "        text = ''.join(random.sample(char_map, num_char))\n",
    "        img = image_generator.generate_image(text)\n",
    "        X[i] = img_preprocess(img)\n",
    "        y_index = [ char_map.find(text[_i])+_i*num_charmap for _i in range(num_char)]\n",
    "        y[i, y_index] = 1\n",
    "#         if (i+1) % (num_images/5) == 0:\n",
    "#             print(\"generating captcha \" , i)\n",
    "    return X, y\n",
    "        #灰度值求解\n",
    "        #np.sum(np.array([0.2989, 0.5870, 0.1140]) * img, axis=2)\n",
    "    \n",
    "def get_next_batch(X, y, batch_size=100):\n",
    "    index_in_epoch = 0\n",
    "    num_images = X.shape[0]\n",
    "#     assert num_images <= X.shape[0]\n",
    "    while True:\n",
    "        start = index_in_epoch\n",
    "        index_in_epoch += batch_size\n",
    "        if index_in_epoch > num_images:\n",
    "            perm = np.arange(num_images)\n",
    "            np.random.shuffle(perm)\n",
    "            X = X[perm]\n",
    "            y = y[perm]\n",
    "            start = 0\n",
    "            index_in_epoch = batch_size\n",
    "            assert batch_size <= num_images\n",
    "        end = index_in_epoch\n",
    "        yield X[start:end], y[start:end]  \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "#加入drop out的模型2W训练集，5K次迭代达到85%训练准确率，测试准确率50%\n",
    "w_alpha=0.01\n",
    "b_alpha=0.1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, height_images*width_images])\n",
    "Y = tf.placeholder(tf.float32, [None, num_char*num_charmap])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout\n",
    "x = tf.reshape(X, shape=[-1, height_images, width_images, 1])\n",
    "\n",
    "conv_dim = 5\n",
    "\n",
    "# 3 conv layer\n",
    "w_c1 = tf.Variable(w_alpha*tf.random_normal([conv_dim, conv_dim, 1, 32]))\n",
    "b_c1 = tf.Variable(b_alpha*tf.random_normal([32]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1))\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "\n",
    "w_c2 = tf.Variable(w_alpha*tf.random_normal([conv_dim, conv_dim, 32, 64]))\n",
    "b_c2 = tf.Variable(b_alpha*tf.random_normal([64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2))\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "w_c3 = tf.Variable(w_alpha*tf.random_normal([conv_dim, conv_dim, 64, 64]))\n",
    "b_c3 = tf.Variable(b_alpha*tf.random_normal([64]))\n",
    "conv3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3))\n",
    "conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "\n",
    "# w_c4 = tf.Variable(w_alpha*tf.random_normal([conv_dim, conv_dim, 64, 64]))\n",
    "# b_c4 = tf.Variable(b_alpha*tf.random_normal([64]))\n",
    "# conv4 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, strides=[1, 1, 1, 1], padding='SAME'), b_c4))\n",
    "# conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# conv4 = tf.nn.dropout(conv4, keep_prob)\n",
    "\n",
    "# Fully connected layer\n",
    "# w_d = tf.Variable(w_alpha*tf.random_normal([10*25*64, 1024]))\n",
    "w_d = tf.Variable(w_alpha*tf.random_normal([5*13*64, 1024]))\n",
    "# w_d = tf.Variable(w_alpha*tf.random_normal([3*7*64, 1024]))\n",
    "b_d = tf.Variable(b_alpha*tf.random_normal([1024]))\n",
    "dense = tf.reshape(conv3, [-1, w_d.get_shape().as_list()[0]])\n",
    "\n",
    "dense = tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))\n",
    "dense = tf.nn.dropout(dense, keep_prob)\n",
    "\n",
    "w_out = tf.Variable(w_alpha*tf.random_normal([1024, num_char*num_charmap]))\n",
    "b_out = tf.Variable(b_alpha*tf.random_normal([num_char*num_charmap]))\n",
    "out = tf.add(tf.matmul(dense, w_out), b_out)\n",
    "\n",
    "y_conv = tf.reshape(out, [-1, num_char, num_charmap])\n",
    "y_ = tf.reshape(Y, [-1, num_char, num_charmap])\n",
    "loss = tf.reduce_mean(tf.concat(0, [tf.nn.softmax_cross_entropy_with_logits(\n",
    "                y_conv[:, _i, :], y_[:, _i, :]) for _i in range(num_char) ]))\n",
    "\n",
    "# loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(out, Y))\n",
    "    # 最后一层用来分类的softmax和sigmoid有什么不同？\n",
    "# optimizer 为了加快训练 learning_rate应该开始大，然后慢慢衰\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "max_idx_p = tf.argmax(y_conv, 2)\n",
    "max_idx_l = tf.argmax(y_, 2)\n",
    "\n",
    "correct_pred = tf.cast(tf.equal(max_idx_p, max_idx_l), tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.reduce_min(correct_pred, axis=1))\n",
    "# correct_pred = tf.equal(max_idx_p, max_idx_l)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training 2\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#batch_size 小会导致多次迭代仍无法收敛\n",
    "#每当训练准确率达到90%，更换训练集，直到测试准确率达到80%\n",
    "#1W-0.028; 3W-0.269\n",
    "\n",
    "batch_size = 128\n",
    "step = 0\n",
    "X_train, y_train = dataset_generator(20000)\n",
    "train_batch = get_next_batch(X_train, y_train, batch_size)\n",
    "\n",
    "X_test, y_test = dataset_generator(500)\n",
    "while True:\n",
    "    X_batch, y_batch = next(train_batch)\n",
    "\n",
    "    if (step)%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={X: X_batch, Y: y_batch, keep_prob: 1.0})\n",
    "        loss_val = loss.eval(feed_dict={X: X_batch, Y: y_batch, keep_prob: 1.0})\n",
    "        test_accuracy = accuracy.eval(feed_dict={X:X_test, Y: y_test, keep_prob: 1.0})\n",
    "        print(time.ctime() ,\": step %d, train accuracy: %g\"%(step, train_accuracy), \";test accuracy: \", test_accuracy)\n",
    "        print(\"loss: \", loss_val)\n",
    "        if step >= 5000:\n",
    "            break\n",
    "    step += 1\n",
    "    sess.run(optimizer, feed_dict={X: X_batch, Y: y_batch, keep_prob: 0.5})\n",
    "\n",
    "    \n",
    "#  无法收敛的元凶:batch_size\n",
    "#     train_step.run(feed_dict={x: X_batch, y_: y_batch, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = dataset_generator(800)\n",
    "# pred = max_idx_p.eval(feed_dict={X: X_test, keep_prob: 1.0})\n",
    "print(\"test accuracy: \",accuracy.eval(feed_dict={X: X_test, Y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ceil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
