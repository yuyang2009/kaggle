{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "from PIL import Image \n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
    "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "r\"\"\"Downloads and converts Flowers data to TFRecords of TF-Example protos.\n",
    "\n",
    "This module downloads the Flowers data, uncompresses it, reads the files\n",
    "that make up the Flowers data and creates two TFRecord datasets: one for train\n",
    "and one for test. Each TFRecord dataset is comprised of a set of TF-Example\n",
    "protocol buffers, each of which contain a single image and label.\n",
    "\n",
    "The script should take about a minute to run.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "# import random\n",
    "import sys\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# The URL where the Flowers data can be downloaded.\n",
    "# _DATA_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "\n",
    "# The number of images in the validation set.\n",
    "_NUM_VALIDATION = 800\n",
    "\n",
    "# Seed for repeatability.\n",
    "_RANDOM_SEED = 0\n",
    "\n",
    "# The number of shards per dataset split.\n",
    "_NUM_SHARDS = 5\n",
    "\n",
    "\n",
    "class ImageReader(object):\n",
    "    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_jpeg(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_jpeg(self, sess, image_data):\n",
    "        image = sess.run(self._decode_jpeg,\n",
    "                         feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "\n",
    "\n",
    "def _get_filenames_and_classes(dataset_dir):\n",
    "    \"\"\"Returns a list of filenames and inferred class names.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: A directory containing a set of subdirectories representing\n",
    "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "\n",
    "    Returns:\n",
    "    A list of image file paths, relative to `dataset_dir` and the list of\n",
    "    subdirectories, representing class names.\n",
    "    \"\"\"\n",
    "    fish_root = os.path.join(dataset_dir, 'train')\n",
    "    directories = []\n",
    "    class_names = []\n",
    "    for filename in os.listdir(fish_root):\n",
    "        path = os.path.join(fish_root, filename)\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "            class_names.append(filename)\n",
    "\n",
    "#     photo_lst = []\n",
    "#     for directory in directories:\n",
    "#         lst_temp = []\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if filename.split('.')[-1] in ['jpg', 'png', 'jepg']:\n",
    "#                 path = os.path.join(directory, filename)\n",
    "#                 lst_temp.append(path)\n",
    "#         photo_lst.append(lst_temp)\n",
    "\n",
    "#     photo_filenames = []\n",
    "#     for i in range(len(photo_lst[0])):\n",
    "#         for j in range(len(photo_lst)):\n",
    "#             index = j%(len(photo_lst[j]))\n",
    "#             photo_filenames.append(photo_lst[j][index])\n",
    "\n",
    "    photo_filenames = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.split('.')[-1] in ['jpg', 'png', 'jpeg']:\n",
    "                path = os.path.join(directory, filename)\n",
    "                photo_filenames.append(path)\n",
    "        \n",
    "    return photo_filenames, sorted(class_names)\n",
    "\n",
    "\n",
    "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "    output_filename = 'fish_%s_%05d-of-%05d.tfrecord' % (\n",
    "        split_name, shard_id, _NUM_SHARDS)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "def _get_bb():\n",
    "    label_files = ['label/bet_labels.json',\n",
    "                   'label/alb_labels.json',\n",
    "                   'label/yft_labels.json',\n",
    "                   'label/dol_labels.json',\n",
    "                   'label/shark_labels.json',\n",
    "                   'label/lag_labels.json',\n",
    "                   'label/other_labels.json']\n",
    "    labels = []\n",
    "    for i in range(len(label_files)):\n",
    "        label_file = label_files[i]\n",
    "        with open(label_file, 'r') as f:\n",
    "            label_json = json.load(f)\n",
    "        # extend 合并dict\n",
    "        labels.extend(label_json)\n",
    "    return labels\n",
    "\n",
    "def _convert_dataset(split_name, filenames, bb_labels, class_names_to_ids, dataset_dir):\n",
    "    \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "\n",
    "    Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "    \"\"\"\n",
    "    assert split_name in ['train', 'validation']\n",
    "\n",
    "    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "    bbox_count = 0    \n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = ImageReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for shard_id in range(_NUM_SHARDS):\n",
    "                output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id)\n",
    "\n",
    "                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                    start_ndx = shard_id * num_per_shard\n",
    "                    end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "                    \n",
    "                    for i in range(start_ndx, end_ndx):\n",
    "                        sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                            i+1, len(filenames), shard_id))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Read the filename:\n",
    "                        image_data = tf.gfile.FastGFile(filenames[i], 'r').read()\n",
    "                        height, width = image_reader.read_image_dims(sess, image_data)\n",
    "                        \n",
    "                        class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "                        class_id = class_names_to_ids[class_name]\n",
    "\n",
    "                        try:\n",
    "                            bb_label_index = next(index for (index, d) in enumerate(bb_labels) \n",
    "                                                  if d[\"filename\"] == filenames[i].split(os.sep)[-1])\n",
    "                            bbox_json = bb_labels[bb_label_index]['annotations']\n",
    "                            lst_ymin = [bbox['y_min'] for bbox in bbox_json]\n",
    "                            lst_xmin = [bbox['x_min'] for bbox in bbox_json]\n",
    "                            lst_ymax = [bbox['y_max'] for bbox in bbox_json]\n",
    "                            lst_xmax = [bbox['x_max'] for bbox in bbox_json]\n",
    "                            lst_label = [int(class_id) for _ in range(len(bbox_json))]\n",
    "                            bbox_count += 1\n",
    "                        except:\n",
    "                            lst_ymin = 0\n",
    "                            lst_xmin = 0\n",
    "                            lst_ymax = 1.0\n",
    "                            lst_xmax = 1.0\n",
    "                            lst_label = 0\n",
    "\n",
    "                        image_name = filenames[i].split(os.sep)[-1]\n",
    "                        example = dataset_utils.image_to_tfexample(\n",
    "                            image_data, b'jpg', height, width, image_name.encode('utf-8'), \n",
    "                            lst_ymin, lst_xmin, lst_ymax, lst_xmax, lst_label)\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "#     print(bbox_count)\n",
    "\n",
    "\n",
    "def _clean_up_temporary_files(dataset_dir):\n",
    "    \"\"\"Removes temporary files used to create the dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: The directory where the temporary files are stored.\n",
    "    \"\"\"\n",
    "    filename = _DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    tf.gfile.Remove(filepath)\n",
    "\n",
    "    tmp_dir = os.path.join(dataset_dir, 'flower_photos')\n",
    "    tf.gfile.DeleteRecursively(tmp_dir)\n",
    "\n",
    "\n",
    "def _dataset_exists(dataset_dir):\n",
    "    for split_name in ['train', 'validation']:\n",
    "        for shard_id in range(_NUM_SHARDS):\n",
    "            output_filename = _get_dataset_filename(\n",
    "                dataset_dir, split_name, shard_id)\n",
    "            if not tf.gfile.Exists(output_filename):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def run(dataset_dir):\n",
    "    \"\"\"Runs the download and conversion operation.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: The dataset directory where the dataset is stored.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "    if _dataset_exists(dataset_dir):\n",
    "        print('Dataset files already exist. Exiting without re-creating them.')\n",
    "        return\n",
    "\n",
    "#     dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
    "    photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n",
    "    \n",
    "    # class_id encode begin 1\n",
    "    class_names_to_ids = dict(zip(class_names, range(1, len(class_names)+1)))\n",
    "\n",
    "    # Divide into train and test:\n",
    "    random.seed(_RANDOM_SEED)\n",
    "    random.shuffle(photo_filenames)\n",
    "    training_filenames = photo_filenames[_NUM_VALIDATION:]\n",
    "    validation_filenames = photo_filenames[:_NUM_VALIDATION]\n",
    "    \n",
    "    bb_labels = _get_bb()\n",
    "\n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset('train', training_filenames, bb_labels, class_names_to_ids, dataset_dir)\n",
    "    _convert_dataset('validation', validation_filenames, bb_labels, class_names_to_ids, dataset_dir)\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    labels_to_class_names = dict(zip(range(1, len(class_names)+1), class_names))\n",
    "    dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n",
    "\n",
    "#     _clean_up_temporary_files(dataset_dir)\n",
    "    print('\\nFinished converting the fish dataset!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 2977/2977 shard 4\n",
      ">> Converting image 800/800 shard 4\n",
      "\n",
      "Finished converting the fish dataset!\n"
     ]
    }
   ],
   "source": [
    "run('input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Provides data for the fish dataset.\n",
    "\n",
    "The dataset scripts used to create the dataset can be found at:\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "_FILE_PATTERN = 'fish_%s_*.tfrecord'\n",
    "\n",
    "SPLITS_TO_SIZES = {'train': 11002, 'validation': 2750, 'test': 1000}\n",
    "\n",
    "_NUM_CLASSES = 8\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'image': 'A color image of varying size.',\n",
    "    'label': 'A single integer between 0 and 7',\n",
    "}\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading fish.\n",
    "\n",
    "    Args:\n",
    "    split_name: A train/validation split name.\n",
    "    dataset_dir: The base directory of the dataset sources.\n",
    "    file_pattern: The file pattern to use when matching the dataset sources.\n",
    "      It is assumed that the pattern contains a '%s' string so that the split\n",
    "      name can be inserted.\n",
    "    reader: The TensorFlow reader type.\n",
    "\n",
    "    Returns:\n",
    "    A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if `split_name` is not a valid train/validation split.\n",
    "    \"\"\"\n",
    "    if split_name not in SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "        file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/class/label': tf.FixedLenFeature(\n",
    "            [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        'image/bbox/ymin': tf.VarLenFeature((), tf.float32),\n",
    "        'image/bbox/xmin': tf.VarLenFeature((), tf.float32),\n",
    "        'image/bbox/ymax': tf.VarLenFeature((), tf.float32),\n",
    "        'image/bbox/xmax': tf.VarLenFeature((), tf.float32),\n",
    "    }\n",
    "\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "        'bbox': slim.tfexample_decoder.BoundingBox(\n",
    "            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/bbox/'),\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    labels_to_names = None\n",
    "    if dataset_utils.has_labels(dataset_dir):\n",
    "        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=reader,\n",
    "        decoder=decoder,\n",
    "        num_samples=SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        labels_to_names=labels_to_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fish_data_dir = 'tmp/fish/dataset'\n",
    "# colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "\n",
    "# with tf.Graph().as_default(): \n",
    "#     dataset = get_split('train', fish_data_dir)\n",
    "#     data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "#         dataset, common_queue_capacity=32, common_queue_min=8)\n",
    "#     image, label, bb_label = data_provider.get(['image', 'label', 'bb_label'])\n",
    "    \n",
    "#     with tf.Session() as sess:    \n",
    "#         with slim.queues.QueueRunners(sess):\n",
    "#             for i in range(5):\n",
    "#                 np_image, np_label, np_bb_label = sess.run([image, label, bb_label])\n",
    "#                 height, width, _ = np_image.shape\n",
    "#                 class_name = name = dataset.labels_to_names[np_label]\n",
    "                \n",
    "# #                 from PIL import Image\n",
    "# #                 Image.fromarray(np_image)\n",
    "                \n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(np_image)\n",
    "#                 currentAxis = plt.gca()\n",
    "#                 j = 0\n",
    "#                 if np_bb_label is not b'':\n",
    "#                     np_bb_label_lst = json.loads((np_bb_label.decode('utf-8')))\n",
    "#                     for rect in np_bb_label_lst:\n",
    "#                         _x, _y, _width, _height = rect['x'], rect['y'], rect['width'], rect['height']\n",
    "#                         coords = (_x*width, _y*height), _width*width, _height*height\n",
    "#                         currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=colors[np_label], linewidth=2))\n",
    "#                         j += 1\n",
    "#                 plt.title('%s, %d x %d' % (name, height, width))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count images without bb_label\n",
    "\n",
    "\n",
    "# filenames, classes = _get_filenames_and_classes('input')\n",
    "# bb_labels = _get_bb()\n",
    "# count = 0\n",
    "# for path in filenames:\n",
    "#     filename = path.split(os.sep)[-1]\n",
    "#     if not any(d['filename'] == filename for d in bb_labels):\n",
    "#         count += 1\n",
    "#         print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "    dataset, common_queue_capacity=batch_size*3,\n",
    "    common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that this may take several minutes!!\n",
    "# Fine-tune pre-trained netoworks.\n",
    "\n",
    "import os\n",
    "\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV3/Logits\", \"InceptionV3/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "    \n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'inception_v3.ckpt'),\n",
    "        variables_to_restore)\n",
    "\n",
    "\n",
    "train_dir = 'tmp/fish/inception_finetuned/'\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "checkpoints_dir = 'tmp/fish/my_checkpoints'\n",
    "batch_size = 16\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('train', fish_data_dir)\n",
    "    \n",
    "    images, _, labels = load_batch(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        logits, _ = inception.inception_v3(images, num_classes=dataset.num_classes, is_training=True)\n",
    "        \n",
    "    # Specify the loss function:\n",
    "    with tf.device('/cpu:0'):\n",
    "        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    # GPU error on incepton-v3 because of motherboard: \"failed to query event: CUDA_ERROR_LAUNCH_FAILED,Unexpected Event status: 1\"\n",
    "\n",
    "#     config = tf.ConfigProto(device_count = {'gpu': 0})\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "# windows 查看tensorflow/contrib/slim/python/slim/learning.py 中的 start_standard_services = False\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        init_fn=get_init_fn(),\n",
    "        number_of_steps=5,\n",
    "        session_config=config,)\n",
    "        \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate model\n",
    "# label balance\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nets import inception\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "batch_size = 32\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "train_dir = 'tmp/fish/inception_finetuned/'\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('validation', fish_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    # what's is_training means?\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        logits, _ = inception.inception_v3(images, num_classes=dataset.num_classes, is_training=False)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    with tf.device('/cpu:0'):\n",
    "        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    log_loss = slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    pred = np.zeros((dataset.num_samples, dataset.num_classes), dtype=np.float64)\n",
    "    truth = np.zeros(dataset.num_samples, dtype=np.int32)\n",
    "#     config = tf.ConfigProto(log_device_placement=True, device_count = {'gpu': 0})\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    fin_log_loss = 0\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        init_fn(sess)\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "                   \n",
    "            for j in range(int(dataset.num_samples/batch_size)):\n",
    "                np_probabilities, np_images_raw, np_labels, np_log_loss = sess.run([probabilities, images_raw, labels, log_loss])\n",
    "                pred[j*batch_size:(j+1)*batch_size, :] = np_probabilities\n",
    "                truth[j*batch_size:(j+1)*batch_size] = np_labels\n",
    "                fin_log_loss += np_log_loss\n",
    "    print(\"total_loss on validation dataset is %.4f\" % (fin_log_loss/(dataset.num_samples/batch_size)))\n",
    "\n",
    "\n",
    "#             np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "#             for i in range(batch_size): \n",
    "#                 image = np_images_raw[i, :, :, :]\n",
    "#                 true_label = np_labels[i]\n",
    "#                 predicted_label = np.argmax(np_probabilities[i, :])\n",
    "#                 predicted_name = dataset.labels_to_names[predicted_label]\n",
    "#                 true_name = dataset.labels_to_names[true_label]\n",
    "\n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(image.astype(np.uint8))\n",
    "#                 plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convnet visualize\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nets import inception\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# image_size = inception.inception_v3.default_image_size\n",
    "image_size = 720\n",
    "batch_size = 8\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "train_dir = 'tmp/fish/inception_finetuned/'\n",
    "\n",
    "convnets = []\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split('validation', fish_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        _convnet, _ = inception.inception_v3_base(images, final_endpoint='MaxPool_5a_3x3')\n",
    "#          _, _convnet = inception.inception_v3_base(images)\n",
    "            \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            init_fn(sess)\n",
    "            np_convnet, np_images_raw = sess.run([_convnet, images_raw])\n",
    "            plt.rcParams['figure.figsize'] = (30.0, 15.0)\n",
    "            for i in range(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                convnet = np_convnet[i]\n",
    "                vis_square(convnet.transpose(2, 0, 1))\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate test dataset\n",
    "\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "r\"\"\"Downloads and converts Flowers data to TFRecords of TF-Example protos.\n",
    "\n",
    "This module downloads the Flowers data, uncompresses it, reads the files\n",
    "that make up the Flowers data and creates two TFRecord datasets: one for train\n",
    "and one for test. Each TFRecord dataset is comprised of a set of TF-Example\n",
    "protocol buffers, each of which contain a single image and label.\n",
    "\n",
    "The script should take about a minute to run.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "# import random\n",
    "import sys\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "\n",
    "# The number of shards per dataset split.\n",
    "_NUM_SHARDS = 1\n",
    "\n",
    "\n",
    "class ImageReader(object):\n",
    "    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initializes function that decodes RGB JPEG data.\n",
    "        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "    def read_image_dims(self, sess, image_data):\n",
    "        image = self.decode_jpeg(sess, image_data)\n",
    "        return image.shape[0], image.shape[1]\n",
    "\n",
    "    def decode_jpeg(self, sess, image_data):\n",
    "        image = sess.run(self._decode_jpeg,\n",
    "                         feed_dict={self._decode_jpeg_data: image_data})\n",
    "        assert len(image.shape) == 3\n",
    "        assert image.shape[2] == 3\n",
    "        return image\n",
    "    \n",
    "def _get_filenames_and_classes_test(dataset_dir):\n",
    "    \"\"\"Returns a list of filenames and inferred class names.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: A directory containing a set of subdirectories representing\n",
    "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "\n",
    "    Returns:\n",
    "    A list of image file paths, relative to `dataset_dir` and the list of\n",
    "    subdirectories, representing class names.\n",
    "    \"\"\"\n",
    "    directory = os.path.join(dataset_dir, 'test_stg1')\n",
    "    photo_filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.split('.')[-1] in ['jpg', 'png', 'jepg']:\n",
    "            path = os.path.join(directory, filename)\n",
    "            photo_filenames.append(path)\n",
    "\n",
    "    return photo_filenames\n",
    "\n",
    "def _get_dataset_filename_test(dataset_dir, shard_id):\n",
    "    output_filename = 'fish_%s_%05d-of-%05d.tfrecord' % (\n",
    "        \"test\", shard_id, _NUM_SHARDS)\n",
    "    return os.path.join(dataset_dir, output_filename)\n",
    "\n",
    "def _convert_dataset_test(filenames, dataset_dir):\n",
    "    \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "\n",
    "    Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "    \"\"\"\n",
    "\n",
    "    num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "        \n",
    "    with tf.Graph().as_default():\n",
    "        image_reader = ImageReader()\n",
    "\n",
    "        with tf.Session('') as sess:\n",
    "            for shard_id in range(_NUM_SHARDS):\n",
    "                output_filename = _get_dataset_filename_test(dataset_dir, shard_id)\n",
    "\n",
    "                with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "                    start_ndx = shard_id * num_per_shard\n",
    "                    end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "                    for i in range(start_ndx, end_ndx):\n",
    "                        sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                            i+1, len(filenames), shard_id))\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Read the filename:\n",
    "                        image_data = tf.gfile.FastGFile(filenames[i], 'r').read()\n",
    "                        height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "\n",
    "                        example = dataset_utils.image_to_tfexample_test(\n",
    "                            image_data, b'jpg', height, width, bytes(filenames[i], 'utf8'))\n",
    "                        tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "\n",
    "def run_test(dataset_dir):\n",
    "    \"\"\"Runs the download and conversion operation.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: The dataset directory where the dataset is stored.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(dataset_dir):\n",
    "        tf.gfile.MakeDirs(dataset_dir)\n",
    "\n",
    "#     if _dataset_exists(dataset_dir):\n",
    "#         print('Dataset files already exist. Exiting without re-creating them.')\n",
    "#         return\n",
    "\n",
    "#     dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n",
    "    photo_filenames = _get_filenames_and_classes_test(dataset_dir)\n",
    "\n",
    "\n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset_test(photo_filenames, dataset_dir)\n",
    "\n",
    "#     _clean_up_temporary_files(dataset_dir)\n",
    "    print('\\nFinished converting the fish dataset!')\n",
    "\n",
    "# run_test('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get test pred\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nets import inception\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "batch_size = 50\n",
    "fish_data_dir = 'tmp/fish/dataset'\n",
    "train_dir = 'tmp/fish/inception_finetuned/'\n",
    "\n",
    "\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Provides data for the fish dataset.\n",
    "\n",
    "The dataset scripts used to create the dataset can be found at:\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import dataset_utils\n",
    "import math\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "_FILE_PATTERN = 'fish_%s_*.tfrecord'\n",
    "\n",
    "SPLITS_TO_SIZES = {'train': 2977, 'validation': 800, 'test': 1000}\n",
    "\n",
    "_NUM_CLASSES = 8\n",
    "\n",
    "_ITEMS_TO_DESCRIPTIONS = {\n",
    "    'image': 'A color image of varying size.',\n",
    "    'image_name': 'Name of images'\n",
    "}\n",
    "\n",
    "\n",
    "def get_split_test(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading fish.\n",
    "\n",
    "    Args:\n",
    "    split_name: A train/validation split name.\n",
    "    dataset_dir: The base directory of the dataset sources.\n",
    "    file_pattern: The file pattern to use when matching the dataset sources.\n",
    "      It is assumed that the pattern contains a '%s' string so that the split\n",
    "      name can be inserted.\n",
    "    reader: The TensorFlow reader type.\n",
    "\n",
    "    Returns:\n",
    "    A `Dataset` namedtuple.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: if `split_name` is not a valid train/validation split.\n",
    "    \"\"\"\n",
    "    if split_name not in SPLITS_TO_SIZES:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "\n",
    "    if not file_pattern:\n",
    "        file_pattern = _FILE_PATTERN\n",
    "        file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='png'),\n",
    "        'image/image_name': tf.FixedLenFeature((), tf.string, default_value=b''),\n",
    "    }\n",
    "\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'image_name': slim.tfexample_decoder.Tensor('image/image_name'),\n",
    "    }\n",
    "\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    labels_to_names = None\n",
    "    if dataset_utils.has_labels(dataset_dir):\n",
    "        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "        data_sources=file_pattern,\n",
    "        reader=reader,\n",
    "        decoder=decoder,\n",
    "        num_samples=SPLITS_TO_SIZES[split_name],\n",
    "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
    "        num_classes=_NUM_CLASSES,\n",
    "        labels_to_names=labels_to_names)\n",
    "\n",
    "\n",
    "def load_batch_test(dataset, batch_size=50, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "#     data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "#         dataset, common_queue_capacity=32,\n",
    "#         common_queue_min=8)\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "    dataset, common_queue_capacity=50, shuffle=False, num_epochs=1,\n",
    "    common_queue_min=8)\n",
    "    image_raw, image_name = data_provider.get(['image', 'image_name'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, image_name = tf.train.batch(\n",
    "          [image, image_raw, image_name],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, image_name\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 40\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = get_split_test('test', fish_data_dir)\n",
    "    images, images_raw, image_name = load_batch_test(dataset, height=image_size, width=image_size, batch_size=batch_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    # what's is_training means?\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        logits, _ = inception.inception_v3(images, num_classes=dataset.num_classes, is_training=False)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    pred = np.zeros((dataset.num_samples, dataset.num_classes), dtype=np.float64)\n",
    "    image_names = []\n",
    "#     _config = tf.ConfigProto(log_device_placement=True, device_count = {'GPU': 0})\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        init_fn(sess)\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for j in range(int(dataset.num_samples/batch_size)):\n",
    "                np_probabilities, np_images_raw, bytes_image_name = sess.run([probabilities, images_raw, image_name])\n",
    "                start = j*batch_size\n",
    "                end = np.minimum(start+batch_size, dataset.num_samples)\n",
    "                pred[start: end, :] = np_probabilities\n",
    "                image_names.extend(bytes_image_name)\n",
    "                \n",
    "print(\"generate test pred done\")\n",
    "#             np_probabilities, np_images_raw = sess.run([probabilities, images_raw])\n",
    "#             for i in range(batch_size): \n",
    "#                 image = np_images_raw[i, :, :, :]\n",
    "#                 predicted_label = np.argmax(np_probabilities[i, :])\n",
    "#                 predicted_name = dataset.labels_to_names[predicted_label]\n",
    "\n",
    "#                 plt.figure()\n",
    "#                 plt.imshow(image.astype(np.uint8))\n",
    "#                 plt.title('Prediction [%s]' % (predicted_name))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_id = [i.decode('utf8').split(os.sep)[-1] for i in image_names]\n",
    "\n",
    "submit = open('submit_inceptv3.csv','w')\n",
    "submit.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "\n",
    "for idx, id_n in enumerate(image_id):\n",
    "    probs=['%.17f' % p for p in list(pred[idx, :])]\n",
    "    submit.write('%s,%s\\n' % (str(image_id[idx]),','.join(probs)))\n",
    "\n",
    "submit.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _get_filenames_and_classes(dataset_dir):\n",
    "    \"\"\"Returns a list of filenames and inferred class names.\n",
    "\n",
    "    Args:\n",
    "    dataset_dir: A directory containing a set of subdirectories representing\n",
    "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "\n",
    "    Returns:\n",
    "    A list of image file paths, relative to `dataset_dir` and the list of\n",
    "    subdirectories, representing class names.\n",
    "    \"\"\"\n",
    "    fish_root = os.path.join(dataset_dir, 'train')\n",
    "    directories = []\n",
    "    class_names = []\n",
    "    for filename in os.listdir(fish_root):\n",
    "        path = os.path.join(fish_root, filename)\n",
    "        if os.path.isdir(path):\n",
    "            directories.append(path)\n",
    "            class_names.append(filename)\n",
    "\n",
    "    photo_filenames = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.split('.')[-1] in ['jpg', 'png', 'jepg']:\n",
    "                path = os.path.join(directory, filename)\n",
    "                photo_filenames.append(path)\n",
    "\n",
    "    return photo_filenames, sorted(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb_labels = _get_bb()\n",
    "filenames, _ = _get_filenames_and_classes('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 6\n",
    "bb_label_index = next(index for (index, d) in enumerate(bb_labels) \n",
    "                      if d[\"filename\"] == filenames[i].split(os.sep)[-1])\n",
    "bbox_json = bb_labels[bb_label_index]['annotations']\n",
    "lst_ymin = [bbox['y_min'] for bbox in bbox_json]\n",
    "lst_xmin = [bbox['x_min'] for bbox in bbox_json]\n",
    "lst_ymax = [bbox['y_max'] for bbox in bbox_json]\n",
    "lst_xmax = [bbox['x_max'] for bbox in bbox_json]\n",
    "\n",
    "# lst_ymin = bbox_lst[:][0]\n",
    "# lst_xmin = bbox_lst[:][1]\n",
    "# lst_ymax = bbox_lst[:][2]\n",
    "# lst_xmax = bbox_lst[:][3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5529843750000084, 0.6047578125000092]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_xmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbox_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
