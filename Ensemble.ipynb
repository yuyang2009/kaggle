{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "titanic_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
    "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\n",
    "test_df    = test_df.drop(['Name','Ticket','Cabin'], axis=1)\n",
    "titanic_df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "test_df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "test_fare_mean = test_df[\"Fare\"].mean()\n",
    "test_df[\"Fare\"].fillna(test_fare_mean, inplace=True)\n",
    "\n",
    "# get average, std, and number of NaN values in titanic_df\n",
    "average_age_titanic   = titanic_df[\"Age\"].mean()\n",
    "std_age_titanic       = titanic_df[\"Age\"].std()\n",
    "count_nan_age_titanic = titanic_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# get average, std, and number of NaN values in test_df\n",
    "average_age_test   = test_df[\"Age\"].mean()\n",
    "std_age_test       = test_df[\"Age\"].std()\n",
    "count_nan_age_test = test_df[\"Age\"].isnull().sum()\n",
    "\n",
    "# generate random numbers between (mean - std) & (mean + std)\n",
    "rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n",
    "rand_2 = np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n",
    "\n",
    "# fill NaN values in Age column with random values generated\n",
    "titanic_df[\"Age\"][np.isnan(titanic_df[\"Age\"])] = rand_1\n",
    "test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = rand_2\n",
    "\n",
    "#transform Sex\n",
    "titanic_df.replace(to_replace=[\"male\", \"female\"], value=[1, 0], inplace=True)\n",
    "titanic_df.replace(to_replace=[\"S\", \"C\", \"Q\"], value=[1, 2, 3], inplace=True)\n",
    "\n",
    "test_df.replace(to_replace=[\"male\", \"female\"], value=[1, 0], inplace=True)\n",
    "test_df.replace(to_replace=[\"S\", \"C\", \"Q\"], value=[1, 2, 3], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# define training and testing sets\n",
    "\n",
    "X = titanic_df.drop(\"Survived\",axis=1).values\n",
    "Y = titanic_df[\"Survived\"].values\n",
    "X_predict  = test_df.drop(\"PassengerId\",axis=1).copy().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75977654,  0.77094972,  0.8258427 ,  0.79775281,  0.85875706])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = make_pipeline(preprocessing.StandardScaler(), MLPClassifier(solver='lbfgs', activation='logistic', alpha=3e-4, hidden_layer_sizes=(2, 400), random_state=1))\n",
    "cross_val_score(clf, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.842696629213 \n",
      "test score: 0.826815642458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=None)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=3, random_state=None).fit(X_train, Y_train)\n",
    "print(\"train score :\", clf.score(X_train, Y_train) , '\\ntest score:', clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pred_scaled = scaler.transform(X_predict)\n",
    "Y_pred = clf.predict(X_pred_scaled)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paramaters select and ensemb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_activation:  [0 2 2 1 1 2 0 0 0 2]\n",
      "accuracy_lst:  [ 0.81564246  0.81564246  0.7877095   0.81564246  0.82681564  0.81564246\n",
      "  0.81564246  0.81564246  0.81564246  0.83240223]\n",
      "dissim_lst:  [ 0.07821229  0.08193669  0.09683426  0.09310987  0.08814401  0.09683426\n",
      "  0.07821229  0.07821229  0.07821229  0.08690255]\n",
      "ensemble_weight:  [ 0.09933531  0.09950499  0.08678939  0.10001564  0.10631815  0.10018642\n",
      "  0.09933531  0.09933531  0.09933531  0.10984419]\n",
      "ensembled_aaccuracy : 0.815642458101 \n",
      "max accuracy: 0.832402234637 \n",
      "ew_rate:  3.50286739903\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "def func_noncorrcoef_weight(prediction_mat):\n",
    "    cc = np.corrcoef(prediction_mat)\n",
    "    cc_scale = 1- (cc + 1)/2\n",
    "    a = np.sum(cc_scale, axis=0) / (cc_scale.shape[0] - 1)\n",
    "    return a\n",
    "\n",
    "def func_ensemble_weight(accuracy_lst, prediction_mat, ac_weight):\n",
    "    aw = np.tan(np.pi * accuracy_lst /2)\n",
    "    cw = func_dissim_weight(prediction_mat)\n",
    "    ew = np.multiply(cw, aw)\n",
    "    return ew / np.sum(ew), ew.mean()\n",
    "\n",
    "def func_ensemble_weight2(accuracy_lst, prediction_mat, ac_weight):\n",
    "    aw = np.tan(np.pi * accuracy_lst /2)\n",
    "    cw = func_dissim_weight(prediction_mat)\n",
    "    cw = np.tan(np.pi * cw / 2)\n",
    "    ew = cw + ac_weight * aw\n",
    "    return ew / np.sum(ew), ew.mean()\n",
    "\n",
    "#caculate the dissimilarty of the predcitons\n",
    "def func_dissim_weight(prediction_mat):\n",
    "    num_prediction = prediction_mat.shape[1]\n",
    "    dissim_mat = distance.squareform(distance.pdist(prediction_mat, 'sqeuclidean')) / num_prediction\n",
    "    return np.sum(dissim_mat, axis=0) / (dissim_mat.shape[0] - 1)\n",
    "\n",
    "X_ensemble, Y_ensemble = X_test, Y_test\n",
    "num_ensemble = Y_ensemble.shape[0]\n",
    "max_loop = 100\n",
    "stop_accuracy = 0.9\n",
    "num_clf = 10\n",
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "no_activation = np.random.randint(0, 4, 10)\n",
    "param_dict = dict(solver='lbfgs', alpha=3e-6, random_state=None)\n",
    "accuracy_lst = np.zeros(num_clf)\n",
    "prediction_mat = np.zeros((num_clf, num_ensemble))\n",
    "clf_lst = []\n",
    "for i in range(num_clf):\n",
    "    nn_size = (np.random.randint(100, 200)\n",
    "               , np.random.randint(100, 200))\n",
    "    nn_activation = activation[no_activation[i]]\n",
    "    clf = MLPClassifier(hidden_layer_sizes = nn_size, activation = nn_activation, **param_dict)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    prediction_mat[i, :] = clf.predict(X_ensemble)\n",
    "    clf_lst.append(clf)\n",
    "    accuracy_lst[i] = clf.score(X_ensemble, Y_ensemble)\n",
    "    \n",
    "dissim_lst = func_dissim_weight(prediction_mat)\n",
    "print(\"no_activation: \", no_activation)\n",
    "print(\"accuracy_lst: \", accuracy_lst)\n",
    "print(\"dissim_lst: \", dissim_lst)\n",
    "ensemble_weight, ew_rate = func_ensemble_weight2(accuracy_lst, prediction_mat, 1)\n",
    "ensemble_pred = np.dot(ensemble_weight, prediction_mat)\n",
    "ensemble_pred[np.argwhere(ensemble_pred > 0.5)] = 1\n",
    "ensemble_pred[np.argwhere(ensemble_pred <= 0.5)] = 0\n",
    "ensembled_accuracy = sklearn.metrics.accuracy_score(Y_ensemble, ensemble_pred)\n",
    "print(\"ensemble_weight: \", ensemble_weight)\n",
    "print(\"ensembled_aaccuracy :\", ensembled_accuracy, \"\\nmax accuracy:\", np.max(accuracy_lst), \n",
    "      \"\\new_rate: \", ew_rate)\n",
    "# for i in range(max_loop):\n",
    "#     if np.any(clf_lst > stop_accuracy):\n",
    "#         print(\"clf no.%d has reach stop accuracy\" % np.argwhere(clf_lst > stop_accuracy))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble_weight:  [ 0.09102714  0.09540891  0.09720413  0.10859532  0.10972793  0.11300555\n",
      "  0.09102714  0.09102714  0.09102714  0.11194958]\n",
      " ensembled_aaccuracy : 0.804469.\n",
      " max accuracy: 0.832402.\n"
     ]
    }
   ],
   "source": [
    "def func_ensemble_weight3(accuracy_lst, prediction_mat, ac_weight):\n",
    "    aw = np.tan(np.pi * accuracy_lst /2)\n",
    "    aw_rate = aw / np.sum(aw)\n",
    "    cw = func_dissim_weight(prediction_mat)\n",
    "    cw = np.tan(np.pi * cw / 2)\n",
    "    ew = cw * aw\n",
    "    return ew / np.sum(ew), ew.mean()\n",
    "ensemble_weight, ew_rate = func_ensemble_weight3(accuracy_lst, prediction_mat, 1e2)\n",
    "ensemble_pred = np.dot(ensemble_weight, prediction_mat)\n",
    "ensemble_pred[np.argwhere(ensemble_pred > 0.5)] = 1\n",
    "ensemble_pred[np.argwhere(ensemble_pred <= 0.5)] = 0\n",
    "ensembled_accuracy = sklearn.metrics.accuracy_score(Y_ensemble, ensemble_pred)\n",
    "print(\"ensemble_weight: \", ensemble_weight)\n",
    "print(\" ensembled_aaccuracy : %f.\\n\" % ensembled_accuracy, \"max accuracy: %f.\" % np.max(accuracy_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.884831460674 \n",
      "test score: 0.787709497207\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', activation='tanh', alpha=3e-5, hidden_layer_sizes=(200, 200), \n",
    "                    verbose=True, max_iter=500 )\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"train score:\", clf.score(X_train, Y_train), \"\\ntest score:\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.900280898876 \n",
      "test score: 0.793296089385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler= preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=None)\n",
    "\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=3, random_state=None)\n",
    "parameters = {'n_estimators':[100, 300, 500], 'max_depth':[1, 3, 5], \"learning_rate\": [0.1, 0.01, 0.03, 0.001]}\n",
    "clf = GridSearchCV(GB_clf, parameters)\n",
    "clf.fit(X_train, Y_train)\n",
    "clf = clf.best_estimator_\n",
    "print(\"train score:\", clf.score(X_train, Y_train), \"\\ntest score:\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81564245810055869"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlf no 0 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 1 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 2 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 3 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 4 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 5 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 6 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 7 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 8 train score : 0.86797752809 test score: 0.837988826816\n",
      "dlf no 9 train score : 0.86797752809 test score: 0.837988826816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "scaler= preprocessing.MinMaxScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=None)\n",
    "\n",
    "num_clf = 10\n",
    "clf_lst = list() \n",
    "for i in range(num_clf):\n",
    "    clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=3, random_state=i).fit(X_train, Y_train)\n",
    "    clf_lst.append(clf)\n",
    "    print(\"dlf no\", i, \"train score :\", clf.score(X_train, Y_train) , 'test score:', clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8904494382022472"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score : 0.842696629213 \n",
      "test score: 0.821229050279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "scaler= preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=None)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.003, max_depth=3, random_state=None).fit(X_train, Y_train)\n",
    "print(\"train score :\", clf.score(X_train, Y_train) , '\\ntest score:', clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf no.0 train score: 0.818820224719 test score: 0.810055865922\n",
      "clf no.1 train score: 0.820224719101 test score: 0.821229050279\n",
      "clf no.2 train score: 0.813202247191 test score: 0.826815642458\n",
      "clf no.3 train score: 0.831460674157 test score: 0.821229050279\n",
      "clf no.4 train score: 0.818820224719 test score: 0.826815642458\n",
      "clf no.5 train score: 0.817415730337 test score: 0.821229050279\n",
      "clf no.6 train score: 0.831460674157 test score: 0.815642458101\n",
      "clf no.7 train score: 0.824438202247 test score: 0.815642458101\n",
      "clf no.8 train score: 0.823033707865 test score: 0.826815642458\n",
      "clf no.9 train score: 0.821629213483 test score: 0.826815642458\n"
     ]
    }
   ],
   "source": [
    "scaler= preprocessing.MinMaxScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=None)\n",
    "\n",
    "num_clf = 10\n",
    "clf_lst = list() \n",
    "for i in range(num_clf):\n",
    "    clf = MLPClassifier(solver='lbfgs', activation='logistic', alpha=3e-6, hidden_layer_sizes=(200, 200), \n",
    "                        verbose=False, max_iter=200 ).fit(X_train, Y_train)\n",
    "    clf_lst.append(clf)\n",
    "    print(\"clf no.%d\" % i, \"train score:\", clf.score(X_train, Y_train), \"test score:\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pred_scaled = scaler.transform(X_predict)\n",
    "Y_pred = clf.predict(X_pred_scaled)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65432099,  0.64758698,  0.36891139,  0.06537598,  0.06359895,\n",
       "        0.06285843,  0.18069585])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
